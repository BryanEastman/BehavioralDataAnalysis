---
title: "R Notebook"
output: html_notebook
---

This is the [R Markdown](http://rmarkdown.rstudio.com) Notebook for chapter 9: Experimental design 3 - cluster randomization and hierarchical modeling

# Data and libraries

```{r}
# Common libraries
suppressMessages(suppressWarnings(library(tidyverse)))
library(rstudioapi)
library(ggpubr)

# Libraries for high-performance Bootstrap
library(Rfast)
library(doParallel)

# Chapter-specific libraries
library(blockTools) # For function block()
library(caret) # For one-hot encoding function dummyVars()
library(scales) # For function rescale()
#library(gtools)
#library(Rlab)
library(lme4) # For hierarchical modeling
library(lmerTest) # For additional diagnostics of hierarchical modeling
library(nbpMatching) #To use 'optimal' algorithm in stratified randomization
#library(binaryLogic)

### Setting the working directory to the parent folder of this script (Rstudio only)
sourceDir <- rstudioapi::getActiveDocumentContext()$path %>% str_extract("^.+/")
setwd(sourceDir)

options(scipen=10)
```

```{r}
#Reading the data
hist_data <- read_csv("chap9-historical_data.csv")
exp_data <- read_csv("chap9-experimental_data.csv")

#Reformating the data
hist_data <- hist_data %>%
  mutate(center_ID = factor(center_ID)) %>%
  mutate(rep_ID = factor(rep_ID)) %>%
  mutate(reason = factor(reason)) %>%
  select(-M6Spend)
exp_data <- exp_data %>%
  mutate(center_ID = factor(center_ID, levels = levels(hist_data$center_ID))) %>%
  mutate(rep_ID = factor(rep_ID, levels = levels(hist_data$rep_ID))) %>%
  mutate(reason = factor(reason, levels = levels(hist_data$reason))) %>%
  mutate(group = factor(group)) %>%
  select(-M6Spend)
```

# Introduction to hierarchical modeling 

```{r}
hlm_mod <- lmer(data=hist_data, call_CSAT ~ reason + age + (1|center_ID))
summary(hlm_mod)
hist_data %>%
  group_by(center_ID)%>%
  summarize(call_CSAT = mean(call_CSAT)) %>%
  summarize(sd = sd(call_CSAT))
```

```{r}
hist_data %>%
  group_by(rep_ID) %>%
  summarise(call_CSAT = mean(call_CSAT)) %>%
  summarise(sd = sd(call_CSAT))
```

```{r}
hlm_mod2 <- lmer(data=hist_data,
                 call_CSAT ~ reason + age + (1|center_ID/rep_ID),
                 control = lmerControl(optimizer ="Nelder_Mead"))
summary(hlm_mod2)
```

# Determining random assignment and sample size/power

### NEED TO CORRECT THE TEXT FIRST ###









#### General Functions ####

hlm_metric_fun <- function(dat){
  #Estimating treatment coefficient with hierarchical regression
  hlm_mod <- lmer(data=dat, 
                  call_CSAT ~ reason + age + group + (1|center_ID/rep_ID)
                  #,control = lmerControl(optimizer ="Nelder_Mead")
                  )
  metric <- fixef(hlm_mod)["grouptreat"]
  return(metric)
}
#hlm_metric_fun(exp_data)

lm_metric_fun <- function(dat){
  #Estimating treatment coefficient with hierarchical regression
  lm_mod <- lm(data = dat, call_CSAT ~ reason + age + group)
  summ <- summary(lm_mod)
  metric <- summ$coefficients["grouptreat", "Estimate"]
  return(metric)
}
#lm_metric_fun(exp_data)

### Bootstrap CI function
boot_CI_fun <- function(dat, metric_fun, B=20, conf.level=0.9){
  
  boot_vec <- sapply(1:B, function(x){
    #cat("bootstrap iteration ", x, "\n")
    metric_fun(slice_sample(dat, n = nrow(dat), replace = TRUE))})
  boot_vec <- sort(boot_vec, decreasing = FALSE)
  offset = round(B * (1 - conf.level) / 2)
  CI <- c(boot_vec[offset], boot_vec[B+1-offset])
  return(CI)
}
#boot_CI_fun(exp_data, hlm_metric_fun)


### decision function
decision_fun <- function(dat, metric_fun, B = 100, conf.level = 0.9){
  boot_CI <- boot_CI_fun(dat, metric_fun, B = B, conf.level = conf.level)
  decision <- ifelse(boot_CI[1]>0,1,0)
  return(decision)
}
#decision_fun(exp_data, hlm_metric_fun, B = 20, conf.level = 0.9)

#### Random assignment ####

### Function to prep the data
strat_prep_fun <- function(dat){
  #Extracting property-level variables
  dat <- dat %>%
    group_by(center_ID) %>%
    summarise(nreps = n_distinct(rep_ID),
              avg_call_CSAT = mean(call_CSAT), 
              avg_age = mean(age),
              pct_reason_pmt = sum(reason == 'payment')/n()) %>%
    ungroup()
  
  #Isolating the different components of our data
  center_ID <- dat$center_ID  # Center identifier
  dat <- dat %>% select(-center_ID)
  num_vars <- dat %>%
    #Selecting numeric variables
    select_if(function(x) is.numeric(x)|is.integer(x)) 
  
  #Normalizing numeric variables
  num_vars_out <- num_vars %>%
    mutate_all(rescale)
  
  #Putting the variables back together
  dat_out <- cbind(center_ID, num_vars_out)  %>%
    mutate(center_ID = as.character(center_ID)) %>%
    mutate_if(is.numeric, function(x) round(x, 4)) #Rounding for readability
  
  return(dat_out)
}
#prepped_data <- strat_prep_fun(hist_data)


#Using a wrapper function to get the stratified pairs
block_wrapper_fun <- function(dat){
  
  prepped_data <- strat_prep_fun(dat)
  
  #Getting stratified assignment
  assgt <- prepped_data %>%
    block(id.vars = c("center_ID"), n.tr = 2, 
          algorithm = "optimal", distance = "euclidean") %>%
    assignment() 
  assgt <- assgt$assg$`1` 
  assgt <- assgt %>%
    select(-'Distance')
  
  assgt <- as.matrix(assgt) %>% apply(2, function(x) as.integer(x))
  return(assgt)
}
#stratified_pairs <- block_wrapper_fun(hist_data)
#stratified_pairs

##### Simulation function #####
power_sim_fun <- function(dat, metric_fun, Nexp = 1000, eff_size = 0, 
                          B = 100, conf.level = 0.9){
  
  #Extract the stratified pairs
  stratified_pairs <- block_wrapper_fun(dat)
  
  Npairs <- nrow(stratified_pairs)
  Nmonths <- length(unique(dat$month))
  Nperm <- 2^Npairs
  Nsim <- Nmonths * Nperm
  
  power_list <- vector(mode = "list", length = Nsim)
  i <- 1
  for(m in unique(dat$month)){
    #Sample down the data
    sample_data <- filter(dat, month==m) %>%
      dplyr::group_by(rep_ID) %>%
      slice_sample(n = Nexp) %>%
      dplyr::ungroup()
    for(perm in 0:(Nperm-1)){
      cat("starting simulation number", i, "\n")
      bin_str <- as.binary(perm, n=Npairs)
      idx <- matrix(c(1:NPairs, bin_str), nrow = Npairs)
      idx[,2] <- idx[,2] + 1
      
      treat <- stratified_pairs[idx]
      
      sim_data <- sample_data %>%
        mutate(group = ifelse(center_ID %in% treat, 'treat', 'ctrl')) %>%
        mutate(group = as.factor(group))
      
      sim_data <- sim_data %>%
        mutate(call_CSAT = ifelse(group == 'treat', 
                                  call_CSAT + eff_size, call_CSAT)) %>%
        #Ensuring that call CSAT remains between 0 and 10
        mutate(call_CSAT = pmax(call_CSAT, 0)) %>%
        mutate(call_CSAT = pmin(call_CSAT, 10))
      
      sim_CI <- boot_CI_fun(sim_data, hlm_metric_fun)
      cat(sim_CI, "\n")
      power_list[[i]] <- sim_CI
      #Calculate the decision
      #power_list[[i]] <- decision_fun(sim_data, metric_fun, B = B, conf.level = conf.level)
      i <- i + 1
    }
  }
  #power <- mean(unlist(power_list))
  return(power_list)
}

# ### Calculating the significance for various confidence levels 
# fun_lst <- c('block_wrapper_fun', 'hlm_metric_fun', 'boot_CI_fun',  'decision_fun', 
#              'power_sim_fun')
# pckg_lst <- c('tidyverse', 'caret', 'binaryLogic', 'lme4', 'lmerTest', 'scales', 'blockTools')
# conf.levels <- c(0.5, 0.9, 0.94, 0.98)
# 
# registerDoParallel()
# sig_res_list <- foreach(conf=conf.levels, .export=fun_lst, .packages=pckg_lst) %dopar% {
#   power_sim_fun(hist_data, hlm_metric_fun, Nexp=1e3, eff_size=0, B = 200, conf.level = conf)
# }
# stopImplicitCluster()
# 
# #Template code to extract the data for one of the confidence levels
# sig_list0.50 <- sig_res_list[1]
# sig_dat0.50 <- tibble(
#   lower_bound = matrix(data = unlist(sig_list0.50), ncol = 2, byrow = TRUE)[,1], 
#   upper_bound = matrix(data = unlist(sig_list0.50), ncol = 2, byrow = TRUE)[,2],
#   conf.level = 0.50,
#   eff_size = 0
# )

#Loading the saved output from the simulations
sig_dat <- read_csv("sig_dat.csv")

ggplot(sig_dat %>% filter(conf.level %in% c(0.5,0.98)), aes(x=lower_bound, y=upper_bound, group = factor(conf.level))) +
  geom_point(alpha = 0.5, aes(color=factor(conf.level))) + 
  geom_abline(intercept = 0, slope = 1) +
  xlim(c(-0.15,0.15)) + ylim(c(-0.15,0.15))

ggplot(sig_dat %>% filter(conf.level==0.90) %>% arrange(lower_bound), aes(y=1:96)) + 
  geom_linerange(aes(xmin=lower_bound, xmax=upper_bound)) +
  geom_vline(xintercept = 0, col='red') +
  ylab("Ordered confidence intervals") +
  xlab("Coefficient for experimental treatment")

### Building the power curve for various effect sizes 

# #Optimized function
# fun_lst <- c('block_wrapper_fun', 'hlm_metric_fun', 'boot_CI_fun',  'decision_fun', 
#              'power_sim_fun')
# pckg_lst <- c('tidyverse', 'caret', 'binaryLogic', 'lme4', 'lmerTest', 'scales', 'blockTools')
# eff_sizes <- c(0.125, 0.25, 0.50, 0.625, 0.75, 0.875, 1)
# 
# registerDoParallel()
# ES_res_list <- foreach(ES=eff_sizes, .export=fun_lst, .packages=pckg_lst) %dopar% {
#   power_sim_fun(hist_data, hlm_metric_fun, Nexp=1e3, eff_size=ES, B = 20, conf.level = 0.9)
# }
# stopImplicitCluster()
# 
# #Template code to extract the data for one of the effect sizes
# ES_list0.125 <- ES_res_list[1]
# ES_dat0.125 <- tibble(
#   lower_bound = matrix(data = unlist(ES_list0.125), ncol = 2, byrow = TRUE)[,1], 
#   upper_bound = matrix(data = unlist(ES_list0.125), ncol = 2, byrow = TRUE)[,2],
#   conf.level = 0.9,
#   eff_size = 0.125
# )

#Loading the saved output from the simulations
ES_dat <- read_csv("ES_dat.csv")


ggplot(ES_dat, aes(x=lower_bound, y=upper_bound)) +
  geom_jitter(alpha = 0.5, aes(color=eff_size)) + 
  geom_abline(intercept = 0, slope = 1)

power_dat <- ES_dat %>%
  group_by(eff_size)  %>%
  summarise(power = sum(lower_bound > 0)/n()) %>%
  ungroup() %>%
  rbind(sig_dat %>% filter(conf.level == 0.90) %>%
          group_by(eff_size)  %>%
          summarise(power = sum(lower_bound > 0)/n()))
power_dat

ggplot(power_dat, aes(x = eff_size, y = power)) +
  geom_point() + ylim(c(0,1)) + xlab("effect size") +
  geom_line(data = power_dat %>% filter(eff_size != 0)) + 
  geom_label(x=0.010, y=0.555, label="significance") +
  geom_hline(yintercept = 0.80, col = 'red')

power_dat2 <- ES_dat %>%
  group_by(eff_size)  %>%
  summarise(power = sum(lower_bound > 0.25)/n()) %>%
  ungroup() %>%
  rbind(sig_dat %>% filter(conf.level == 0.90) %>%
          group_by(eff_size)  %>%
          summarise(power = sum(lower_bound > 0.25)/n()))
power_dat2

ggplot(power_dat2, aes(x = eff_size, y = power)) +
  geom_point() + ylim(c(0,1)) + xlab("effect size") +
  geom_line(data = power_dat2 %>% filter(eff_size != 0)) + 
  geom_label(x=0.010, y=0.355, label="significance") +
  geom_hline(yintercept = 0.80, col = 'red')

##### Analyzing experimental data #####

# coeff <- hlm_metric_fun(exp_data)
# print(coeff)
# 
# hlm_CI <- boot_CI_fun(exp_data, hlm_metric_fun)
# print(hlm_CI)